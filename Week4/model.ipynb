{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.layers import Dense, Input, Embedding, LayerNormalization, Dropout\n",
        "from tensorflow.keras.models import Model\n",
        "import numpy as np\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def load_text(path: str) -> str:\n",
        "    \"\"\"Load raw text and do minimal preprocessing.\"\"\"\n",
        "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
        "        data = f.read().replace(\"\\n\", \" \")\n",
        "    return data\n",
        "\n",
        "data = load_text(\"training_data.txt\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(len(data))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def build_vocab(data: str):\n",
        "    \"\"\"\n",
        "    Build character set + integer encodings.\n",
        "\n",
        "    Convention (same spirit as your original):\n",
        "    - Reserve 0 optionally for padding/unknown (you decide).\n",
        "    - Map characters to 1..V-1.\n",
        "    \"\"\"\n",
        "    characters = list(set(list(data)))\n",
        "    vocab_size = len(characters) + 1\n",
        "\n",
        "    char2idx = {}\n",
        "    idx2char = {}\n",
        "    for i, ch in enumerate(characters):\n",
        "        char2idx[ch] = i + 1\n",
        "        idx2char[i + 1] = ch\n",
        "\n",
        "    return characters, vocab_size, char2idx, idx2char\n",
        "\n",
        "characters, input_vocab_size, character_to_integer_encoding, integer_to_character_encoding = build_vocab(data)\n",
        "print(len(characters))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def encode(string: str):\n",
        "    \"\"\"Convert string -> list of token ids.\"\"\"\n",
        "    global character_to_integer_encoding\n",
        "    return [character_to_integer_encoding[ch] for ch in string]\n",
        "\n",
        "def decode(lst):\n",
        "    \"\"\"Convert list of token ids -> string.\"\"\"\n",
        "    global integer_to_character_encoding\n",
        "    return \"\".join(integer_to_character_encoding[i] for i in lst)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "input_data = encode(data)\n",
        "train_data = input_data[:int(0.9 * len(input_data))]\n",
        "test_data  = input_data[int(0.9 * len(input_data)):]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "batch_size = 32\n",
        "block_size = 128\n",
        "num_heads = 8\n",
        "num_transformer_blocks = 4\n",
        "embed_dim = 256\n",
        "feed_forward_dim = 256\n",
        "dropout_rate = 0.1\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def causal_attention_mask(batch_size, n_dest, n_src):\n",
        "    \"\"\"\n",
        "    Create a causal (lower-triangular) attention mask so position i cannot attend to j > i.\n",
        "    Returns a boolean mask where True means \"allowed to attend\".\n",
        "    \"\"\"\n",
        "    # Create a lower triangular matrix of ones\n",
        "    # Shape: (n_dest, n_src)\n",
        "    mask = tf.linalg.band_part(tf.ones((n_dest, n_src)), -1, 0)\n",
        "    # Convert to boolean (True = attend, False = mask out)\n",
        "    mask = tf.cast(mask, dtype=tf.bool)\n",
        "    return mask\n",
        "\n",
        "\n",
        "class TransformerBlock(layers.Layer):\n",
        "    \"\"\"\n",
        "    One transformer block:\n",
        "    - Causal self-attention\n",
        "    - Residual + LayerNorm\n",
        "    - Feed-forward MLP\n",
        "    - Residual + LayerNorm\n",
        "    \"\"\"\n",
        "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):\n",
        "        super().__init__()\n",
        "\n",
        "        # Multi-head self-attention layer\n",
        "        self.att = layers.MultiHeadAttention(\n",
        "            num_heads=num_heads,\n",
        "            key_dim=embed_dim // num_heads\n",
        "        )\n",
        "\n",
        "        # Feed-forward network: expand then project back\n",
        "        self.ffn = tf.keras.Sequential([\n",
        "            Dense(ff_dim, activation='gelu'),\n",
        "            Dense(embed_dim)\n",
        "        ])\n",
        "\n",
        "        self.normalization_layer_1 = LayerNormalization(epsilon=1e-6)\n",
        "        self.normalization_layer_2 = LayerNormalization(epsilon=1e-6)\n",
        "        self.dropout1 = Dropout(rate)\n",
        "        self.dropout2 = Dropout(rate)\n",
        "\n",
        "    def call(self, inputs, training=False):\n",
        "        \"\"\"\n",
        "        inputs: (batch, seq_len, embed_dim)\n",
        "        \"\"\"\n",
        "        batch_size = tf.shape(inputs)[0]\n",
        "        seq_len = tf.shape(inputs)[1]\n",
        "\n",
        "        # Build causal mask for autoregressive attention\n",
        "        causal_mask = causal_attention_mask(batch_size, seq_len, seq_len)\n",
        "\n",
        "        # Self-attention with causal mask\n",
        "        attn_output = self.att(\n",
        "            query=inputs,\n",
        "            value=inputs,\n",
        "            key=inputs,\n",
        "            attention_mask=causal_mask,\n",
        "            training=training\n",
        "        )\n",
        "        attn_output = self.dropout1(attn_output, training=training)\n",
        "\n",
        "        # Residual connection + layer normalization\n",
        "        out1 = self.normalization_layer_1(inputs + attn_output)\n",
        "\n",
        "        # Feed-forward network\n",
        "        ffn_output = self.ffn(out1)\n",
        "        ffn_output = self.dropout2(ffn_output, training=training)\n",
        "\n",
        "        # Residual connection + layer normalization\n",
        "        out2 = self.normalization_layer_2(out1 + ffn_output)\n",
        "\n",
        "        return out2\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class TokenAndPositionEmbedding(layers.Layer):\n",
        "    \"\"\"Embeds tokens + positions and adds them.\"\"\"\n",
        "    def __init__(self, maxlen, vocab_size, embed_dim):\n",
        "        super().__init__()\n",
        "        self.token_embedding = Embedding(input_dim=vocab_size, output_dim=embed_dim)\n",
        "        self.pos_embedding = Embedding(input_dim=maxlen, output_dim=embed_dim)\n",
        "        self.maxlen = maxlen\n",
        "\n",
        "    def call(self, x):\n",
        "        \"\"\"\n",
        "        x: (batch, seq_len)\n",
        "        Returns: (batch, seq_len, embed_dim)\n",
        "        \"\"\"\n",
        "        seq_len = tf.shape(x)[1]\n",
        "        \n",
        "        # Create position indices [0, 1, 2, ..., seq_len-1]\n",
        "        positions = tf.range(start=0, limit=seq_len, delta=1)\n",
        "        \n",
        "        # Get position embeddings: (seq_len, embed_dim)\n",
        "        pos_emb = self.pos_embedding(positions)\n",
        "        \n",
        "        # Get token embeddings: (batch, seq_len, embed_dim)\n",
        "        tok_emb = self.token_embedding(x)\n",
        "        \n",
        "        # Add token and position embeddings (pos_emb broadcasts over batch)\n",
        "        return tok_emb + pos_emb\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_transformer_model(\n",
        "    maxlen,\n",
        "    vocab_size,\n",
        "    embed_dim,\n",
        "    num_heads,\n",
        "    feed_forward_dim,\n",
        "    num_transformer_blocks=1,\n",
        "    rate=0.1,\n",
        "):\n",
        "    \"\"\"Functional API model builder for causal next-token prediction.\"\"\"\n",
        "    inputs = Input(shape=(maxlen,), dtype=tf.int32)\n",
        "    x = TokenAndPositionEmbedding(maxlen, vocab_size, embed_dim)(inputs)\n",
        "\n",
        "    for _ in range(num_transformer_blocks):\n",
        "        x = TransformerBlock(embed_dim, num_heads, feed_forward_dim, rate=rate)(x)\n",
        "\n",
        "    outputs = Dense(vocab_size)(x)\n",
        "    model = Model(inputs=inputs, outputs=[outputs])\n",
        "    return model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model = get_transformer_model(block_size, input_vocab_size, embed_dim, num_heads, feed_forward_dim, num_transformer_blocks, rate=dropout_rate)\n",
        "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "model.compile(\n",
        "    optimizer=\"adam\",\n",
        "    loss=[loss_fn],\n",
        "    metrics=[\"accuracy\"],\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def make_next_token_dataset(token_ids, block_size):\n",
        "    \"\"\"\n",
        "    inputs[i]  = token_ids[i : i+block_size]\n",
        "    targets[i] = token_ids[i+1 : i+block_size+1]\n",
        "    \"\"\"\n",
        "    inputs = [token_ids[i:i+block_size] for i in range(0, len(token_ids) - block_size - 1)]\n",
        "    targets = [token_ids[i+1:i+block_size+1] for i in range(0, len(token_ids) - block_size - 1)]\n",
        "    return inputs, targets\n",
        "\n",
        "\n",
        "def build_tf_dataset(inputs, targets, batch_size, shuffle_buffer=10000):\n",
        "    \"\"\"\n",
        "    Build a tf.data.Dataset from input/target sequences.\n",
        "    \"\"\"\n",
        "    # Convert to numpy arrays with int32 dtype\n",
        "    X = np.array(inputs, dtype=np.int32)\n",
        "    Y = np.array(targets, dtype=np.int32)\n",
        "    \n",
        "    # Create dataset from tensor slices\n",
        "    dataset = tf.data.Dataset.from_tensor_slices((X, Y))\n",
        "    \n",
        "    # Shuffle the dataset\n",
        "    dataset = dataset.shuffle(buffer_size=shuffle_buffer)\n",
        "    \n",
        "    # Batch the dataset, dropping incomplete final batch\n",
        "    dataset = dataset.batch(batch_size, drop_remainder=True)\n",
        "    \n",
        "    # Prefetch for performance\n",
        "    dataset = dataset.prefetch(tf.data.AUTOTUNE)\n",
        "    \n",
        "    return dataset\n",
        "\n",
        "\n",
        "inputs, targets = make_next_token_dataset(train_data, block_size)\n",
        "\n",
        "# Create the training dataset\n",
        "dataset = build_tf_dataset(inputs, targets, batch_size=batch_size, shuffle_buffer=10000)\n",
        "print(f\"Dataset created with {len(inputs)} samples\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model.summary()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Training the Transformer model\n",
        "# Using a custom callback to display generated text during training\n",
        "\n",
        "class TextGenerationCallback(tf.keras.callbacks.Callback):\n",
        "    def __init__(self, seed_tokens, generate_fn, decode_fn, every_n_epochs=5):\n",
        "        super().__init__()\n",
        "        self.seed_tokens = seed_tokens\n",
        "        self.generate_fn = generate_fn\n",
        "        self.decode_fn = decode_fn\n",
        "        self.every_n_epochs = every_n_epochs\n",
        "    \n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        if (epoch + 1) % self.every_n_epochs == 0:\n",
        "            print(f\"\\n--- Sample generation at epoch {epoch + 1} ---\")\n",
        "            generated = self.generate_fn(self.model, self.seed_tokens.copy(), num_generate=100)\n",
        "            print(generated[:200])\n",
        "            print(\"---\\n\")\n",
        "\n",
        "# Train the model\n",
        "print(\"Starting training...\")\n",
        "history = model.fit(\n",
        "    dataset,\n",
        "    epochs=15,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "print(\"\\nTraining complete!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def generate_text(model, seed_tokens, num_generate=200, temperature=0.8):\n",
        "    \"\"\"\n",
        "    Generate text given seed_tokens using the trained model.\n",
        "    \n",
        "    Args:\n",
        "        model: Trained transformer model\n",
        "        seed_tokens: List of token ids to start generation (length should be block_size)\n",
        "        num_generate: Number of new tokens to generate\n",
        "        temperature: Sampling temperature (higher = more random, lower = more deterministic)\n",
        "    \n",
        "    Returns:\n",
        "        Generated string\n",
        "    \"\"\"\n",
        "    # Ensure seed_tokens has the right length\n",
        "    if len(seed_tokens) < block_size:\n",
        "        # Pad with zeros if too short\n",
        "        seed_tokens = [0] * (block_size - len(seed_tokens)) + seed_tokens\n",
        "    elif len(seed_tokens) > block_size:\n",
        "        # Take the last block_size tokens\n",
        "        seed_tokens = seed_tokens[-block_size:]\n",
        "    \n",
        "    generated_tokens = []\n",
        "    current_tokens = list(seed_tokens)\n",
        "    \n",
        "    for _ in range(num_generate):\n",
        "        # Prepare input tensor\n",
        "        input_eval = tf.convert_to_tensor([current_tokens], dtype=tf.int32)\n",
        "        \n",
        "        # Get model predictions\n",
        "        logits = model(input_eval, training=False)  # (1, block_size, vocab_size)\n",
        "        \n",
        "        # Get logits for the last position\n",
        "        next_token_logits = logits[0, -1, :]  # (vocab_size,)\n",
        "        \n",
        "        # Apply temperature scaling\n",
        "        scaled_logits = next_token_logits / temperature\n",
        "        \n",
        "        # Convert to probabilities using softmax\n",
        "        probs = tf.nn.softmax(scaled_logits).numpy()\n",
        "        \n",
        "        # Sample from the probability distribution\n",
        "        next_token = np.random.choice(len(probs), p=probs)\n",
        "        \n",
        "        # Append to generated tokens\n",
        "        generated_tokens.append(next_token)\n",
        "        \n",
        "        # Update current_tokens: shift left and append new token\n",
        "        current_tokens = current_tokens[1:] + [next_token]\n",
        "    \n",
        "    # Decode and return the generated text\n",
        "    return decode(generated_tokens)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test text generation after training\n",
        "print(\"=\"*60)\n",
        "print(\"TEXT GENERATION DEMO\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Use the beginning of training data as seed\n",
        "seed = train_data[:block_size]\n",
        "seed_text = decode(seed)\n",
        "\n",
        "print(f\"\\nSeed text (first {block_size} characters):\")\n",
        "print(\"-\"*40)\n",
        "print(seed_text)\n",
        "print(\"-\"*40)\n",
        "\n",
        "# Generate text with different temperatures\n",
        "for temp in [0.5, 0.8, 1.0]:\n",
        "    print(f\"\\n\\nGenerated text (temperature={temp}):\")\n",
        "    print(\"-\"*40)\n",
        "    generated = generate_text(model, seed.copy(), num_generate=300, temperature=temp)\n",
        "    print(seed_text + generated)\n",
        "    print(\"-\"*40)\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"GENERATION COMPLETE\")\n",
        "print(\"=\"*60)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize training history\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "# Plot loss\n",
        "axes[0].plot(history.history['loss'], label='Training Loss', color='blue')\n",
        "axes[0].set_xlabel('Epoch')\n",
        "axes[0].set_ylabel('Loss')\n",
        "axes[0].set_title('Training Loss Over Epochs')\n",
        "axes[0].legend()\n",
        "axes[0].grid(True, alpha=0.3)\n",
        "\n",
        "# Plot accuracy\n",
        "axes[1].plot(history.history['accuracy'], label='Training Accuracy', color='green')\n",
        "axes[1].set_xlabel('Epoch')\n",
        "axes[1].set_ylabel('Accuracy')\n",
        "axes[1].set_title('Training Accuracy Over Epochs')\n",
        "axes[1].legend()\n",
        "axes[1].grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Interactive text generation with custom prompts\n",
        "def generate_from_prompt(prompt, num_chars=200, temperature=0.8):\n",
        "    \"\"\"Generate text starting from a custom prompt.\"\"\"\n",
        "    # Encode the prompt\n",
        "    prompt_tokens = encode(prompt)\n",
        "    \n",
        "    # Pad or truncate to block_size\n",
        "    if len(prompt_tokens) < block_size:\n",
        "        prompt_tokens = [0] * (block_size - len(prompt_tokens)) + prompt_tokens\n",
        "    else:\n",
        "        prompt_tokens = prompt_tokens[-block_size:]\n",
        "    \n",
        "    # Generate\n",
        "    generated = generate_text(model, prompt_tokens, num_generate=num_chars, temperature=temperature)\n",
        "    \n",
        "    return prompt + generated\n",
        "\n",
        "# Example prompts in Shakespearean style\n",
        "prompts = [\n",
        "    \"To be or not to be\",\n",
        "    \"All the world's a stage\",\n",
        "    \"Friends, Romans, countrymen\",\n",
        "]\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"CUSTOM PROMPT GENERATION\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "for prompt in prompts:\n",
        "    print(f\"\\nPrompt: '{prompt}'\")\n",
        "    print(\"-\"*40)\n",
        "    result = generate_from_prompt(prompt, num_chars=150, temperature=0.7)\n",
        "    print(result)\n",
        "    print(\"-\"*40)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "usr",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
