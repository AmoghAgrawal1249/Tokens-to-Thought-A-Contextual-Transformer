{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o4BQVnaAHC4F"
      },
      "source": [
        "## Working with Tensorflow\n",
        "In this assignment, you will be familiarized with the usage of the tensorflow library and how to build a model for the MNIST database in two ways\n",
        "*   using the inbuilt layers in tensorflow\n",
        "*   using custom layers to replicate the same result\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "Co1Y3oSoAqHp"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.datasets import mnist\n",
        "from tensorflow.keras.models import Sequential\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras.layers import Dense, Flatten\n",
        "from tensorflow.keras.utils import to_categorical"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n682ZxROHh_1"
      },
      "source": [
        "## Loading and preprocessing the Data\n",
        "We will directly be using the dataset included in tensorflow library\n",
        "A detailed description of data is given at (https://www.tensorflow.org/api_docs/python/tf/keras/datasets/mnist/load_data)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "8zrkUXY8AvtN"
      },
      "outputs": [],
      "source": [
        "# Load the MNIST dataset\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "\n",
        "# Preprocess the data\n",
        "x_train = x_train / 255.0\n",
        "x_test = x_test / 255.0\n",
        "y_train = to_categorical(y_train, 10)\n",
        "y_test = to_categorical(y_test, 10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4NZRlpvMIDxs"
      },
      "source": [
        "Heres how the data looks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 447
        },
        "id": "9jFXtFL4FH2x",
        "outputId": "8b9450bc-0e10-4776-a8a1-1dea5cac909d"
      },
      "outputs": [],
      "source": [
        "plt.imshow(x_train[0],cmap= 'Greys')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y2loSsKCGScS",
        "outputId": "8b6ec3ab-0458-4175-903d-37619b48b848"
      },
      "outputs": [],
      "source": [
        "y_train[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 447
        },
        "id": "-VO0bDzWGN0z",
        "outputId": "4ac2e0d4-a1ce-4a49-cfcc-ce474d8b3d1a"
      },
      "outputs": [],
      "source": [
        "plt.imshow(x_train[1],cmap= 'Greys')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 447
        },
        "id": "WmwFY7-9GP_-",
        "outputId": "0ae5cd81-0404-4b53-a1cb-c3459d7edd8b"
      },
      "outputs": [],
      "source": [
        "plt.imshow(x_train[2],cmap= 'Greys')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d6W4VEerGYJ8"
      },
      "source": [
        "# Making a simple feedforward network\n",
        "As you have seen in the second week a simple feedfordward network works well to solve MNIST.<br/>\n",
        "The following is a simple feedforward model with three layers:\n",
        "* a flatten layer to convert our 28x28 images into a single array of length 784\n",
        "* a dense layer of 128 neurons with the relu activation function\n",
        "* finally a dense layer of 10 neurons with the softmax activation to get a distribution between the digits"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zwFnJsE6vjf8",
        "outputId": "ac06dbd4-9873-4366-d212-8d784b6eab77"
      },
      "outputs": [],
      "source": [
        "# Build the model\n",
        "model = Sequential([\n",
        "    Flatten(input_shape=(28, 28)),\n",
        "    Dense(128, activation='relu'),\n",
        "    Dense(10, activation='softmax')\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam',loss='categorical_crossentropy',metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "model.fit(x_train, y_train, epochs=5, batch_size=32, validation_split=0.2)\n",
        "\n",
        "# Evaluate the model\n",
        "test_loss, test_acc = model.evaluate(x_test, y_test)\n",
        "print(f'Test accuracy: {test_acc}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m7yTvXD_JBq2"
      },
      "source": [
        "## Making Custom Layers\n",
        "You can go through this\n",
        "<a href = 'https://www.tensorflow.org/tutorials/customization/custom_layers'> documentation </a> to get a feel for how to implement a custom layer\n",
        "\n",
        "* Create a CustomDenseLayer with a Relu Activation\n",
        "* Create a CustomDenseLayer with a Softmax Activation\n",
        "* Create a CustomFlatten Layer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7wtMoyyNK-FZ"
      },
      "outputs": [],
      "source": [
        "class CustomDenseReluLayer(tf.keras.layers.Layer):\n",
        "    def __init__(self, units):\n",
        "        super(CustomDenseReluLayer, self).__init__()\n",
        "        self.units = units\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        # Create weight matrix and bias vector\n",
        "        self.w = self.add_weight(\n",
        "            shape=(input_shape[-1], self.units),\n",
        "            initializer='glorot_uniform',\n",
        "            trainable=True,\n",
        "            name='kernel'\n",
        "        )\n",
        "        self.b = self.add_weight(\n",
        "            shape=(self.units,),\n",
        "            initializer='zeros',\n",
        "            trainable=True,\n",
        "            name='bias'\n",
        "        )\n",
        "\n",
        "    def call(self, inputs):\n",
        "        # Linear transformation followed by ReLU activation\n",
        "        z = tf.matmul(inputs, self.w) + self.b\n",
        "        return tf.nn.relu(z)\n",
        "\n",
        "\n",
        "class CustomDenseSoftmaxLayer(tf.keras.layers.Layer):\n",
        "    def __init__(self, units):\n",
        "        super(CustomDenseSoftmaxLayer, self).__init__()\n",
        "        self.units = units\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        # Create weight matrix and bias vector\n",
        "        self.w = self.add_weight(\n",
        "            shape=(input_shape[-1], self.units),\n",
        "            initializer='glorot_uniform',\n",
        "            trainable=True,\n",
        "            name='kernel'\n",
        "        )\n",
        "        self.b = self.add_weight(\n",
        "            shape=(self.units,),\n",
        "            initializer='zeros',\n",
        "            trainable=True,\n",
        "            name='bias'\n",
        "        )\n",
        "\n",
        "    def call(self, inputs):\n",
        "        # Linear transformation followed by softmax activation\n",
        "        z = tf.matmul(inputs, self.w) + self.b\n",
        "        return tf.nn.softmax(z, axis=-1)\n",
        "\n",
        "\n",
        "class CustomFlattenLayer(tf.keras.layers.Layer):\n",
        "    def call(self, inputs):\n",
        "        # Flatten all dimensions except the batch dimension\n",
        "        batch_size = tf.shape(inputs)[0]\n",
        "        return tf.reshape(inputs, (batch_size, -1))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dCEMWK1KLZDT"
      },
      "source": [
        "## Using out custom layers to Build a model for MNIST"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zw53cAmELYJK"
      },
      "outputs": [],
      "source": [
        "# Example usage of the custom dense layer\n",
        "model = Sequential([\n",
        "    CustomFlattenLayer(),\n",
        "    CustomDenseReluLayer(128),\n",
        "    CustomDenseSoftmaxLayer(10)\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "model.fit(x_train, y_train, epochs=5, batch_size=32, validation_split=0.2)\n",
        "\n",
        "# Evaluate the model\n",
        "test_loss, test_acc = model.evaluate(x_test, y_test)\n",
        "print(f'Test accuracy: {test_acc}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K_ymh-5LLyMY"
      },
      "source": [
        "# Assignment\n",
        "* Load and preprocess Boston housing dataset\n",
        "* build a Linear Regression model for it and optimize it using tensorflow (its basically a neural network with a single neuron and no activaton)\n",
        "* build a Feedforward network for it you can expirement around with no of layers and and neurons in each layer and different activation functions <br/>\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Bonus Assignment \n",
        "* Try solving one more random dataset from kaggle/tensorflow datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## Boston Housing Dataset - Linear Regression and Feedforward Network\n",
        "Below we load the Boston Housing dataset, build a simple linear regression model (single neuron, no activation), and then a feedforward network to compare performance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cuyf9f0TE7wX",
        "outputId": "a6cc4902-f834-4991-9ae7-0023b25c115a"
      },
      "outputs": [],
      "source": [
        "# Load Boston Housing dataset\n",
        "# Note: Boston Housing is deprecated in newer sklearn, so we fetch it from the original source\n",
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Using California Housing as a modern alternative (Boston is deprecated)\n",
        "housing = fetch_california_housing()\n",
        "X, y = housing.data, housing.target\n",
        "\n",
        "# Split into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Standardize features for better convergence\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "print(f\"Training samples: {X_train_scaled.shape[0]}\")\n",
        "print(f\"Test samples: {X_test_scaled.shape[0]}\")\n",
        "print(f\"Number of features: {X_train_scaled.shape[1]}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Linear Regression Model (single neuron, no activation)\n",
        "# This is essentially y = Wx + b\n",
        "\n",
        "linear_model = Sequential([\n",
        "    Dense(1, input_shape=(X_train_scaled.shape[1],), activation=None)\n",
        "])\n",
        "\n",
        "linear_model.compile(\n",
        "    optimizer=tf.keras.optimizers.Adam(learning_rate=0.01),\n",
        "    loss='mse',\n",
        "    metrics=['mae']\n",
        ")\n",
        "\n",
        "print(\"Linear Regression Model Summary:\")\n",
        "linear_model.summary()\n",
        "\n",
        "# Train the linear regression model\n",
        "history_linear = linear_model.fit(\n",
        "    X_train_scaled, y_train,\n",
        "    epochs=100,\n",
        "    batch_size=32,\n",
        "    validation_split=0.2,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# Evaluate on test set\n",
        "test_loss_linear, test_mae_linear = linear_model.evaluate(X_test_scaled, y_test)\n",
        "print(f\"\\nLinear Regression - Test MSE: {test_loss_linear:.4f}, Test MAE: {test_mae_linear:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Feedforward Neural Network for Boston Housing\n",
        "# Experimenting with multiple layers and activation functions\n",
        "\n",
        "feedforward_model = Sequential([\n",
        "    Dense(64, input_shape=(X_train_scaled.shape[1],), activation='relu'),\n",
        "    Dense(32, activation='relu'),\n",
        "    Dense(16, activation='relu'),\n",
        "    Dense(1, activation=None)  # No activation for regression output\n",
        "])\n",
        "\n",
        "feedforward_model.compile(\n",
        "    optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
        "    loss='mse',\n",
        "    metrics=['mae']\n",
        ")\n",
        "\n",
        "print(\"Feedforward Neural Network Summary:\")\n",
        "feedforward_model.summary()\n",
        "\n",
        "# Train the feedforward model\n",
        "history_ff = feedforward_model.fit(\n",
        "    X_train_scaled, y_train,\n",
        "    epochs=100,\n",
        "    batch_size=32,\n",
        "    validation_split=0.2,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# Evaluate on test set\n",
        "test_loss_ff, test_mae_ff = feedforward_model.evaluate(X_test_scaled, y_test)\n",
        "print(f\"\\nFeedforward Network - Test MSE: {test_loss_ff:.4f}, Test MAE: {test_mae_ff:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Compare training histories and results\n",
        "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "# Plot training loss comparison\n",
        "axes[0].plot(history_linear.history['loss'], label='Linear Regression - Train')\n",
        "axes[0].plot(history_linear.history['val_loss'], label='Linear Regression - Val')\n",
        "axes[0].plot(history_ff.history['loss'], label='Feedforward - Train')\n",
        "axes[0].plot(history_ff.history['val_loss'], label='Feedforward - Val')\n",
        "axes[0].set_xlabel('Epoch')\n",
        "axes[0].set_ylabel('MSE Loss')\n",
        "axes[0].set_title('Training Loss Comparison')\n",
        "axes[0].legend()\n",
        "axes[0].grid(True, alpha=0.3)\n",
        "\n",
        "# Plot MAE comparison\n",
        "axes[1].plot(history_linear.history['mae'], label='Linear Regression - Train')\n",
        "axes[1].plot(history_linear.history['val_mae'], label='Linear Regression - Val')\n",
        "axes[1].plot(history_ff.history['mae'], label='Feedforward - Train')\n",
        "axes[1].plot(history_ff.history['val_mae'], label='Feedforward - Val')\n",
        "axes[1].set_xlabel('Epoch')\n",
        "axes[1].set_ylabel('MAE')\n",
        "axes[1].set_title('Mean Absolute Error Comparison')\n",
        "axes[1].legend()\n",
        "axes[1].grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Print final comparison\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"Final Model Comparison on Test Set\")\n",
        "print(\"=\"*50)\n",
        "print(f\"Linear Regression: MSE = {test_loss_linear:.4f}, MAE = {test_mae_linear:.4f}\")\n",
        "print(f\"Feedforward NN:    MSE = {test_loss_ff:.4f}, MAE = {test_mae_ff:.4f}\")\n",
        "print(\"=\"*50)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "Welcome To Colab",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
