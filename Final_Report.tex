\documentclass[11pt,a4paper]{article}
\usepackage[margin=1in]{geometry}
\usepackage{amsmath,amssymb}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{hyperref}
\usepackage{listings}
\usepackage{xcolor}
\usepackage{fancyhdr}
\usepackage{titlesec}
\usepackage{enumitem}

% Code listing style
\definecolor{codegreen}{rgb}{0,0.6,0}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{backcolour}{rgb}{0.95,0.95,0.92}

\lstdefinestyle{mystyle}{
    backgroundcolor=\color{backcolour},
    commentstyle=\color{codegreen},
    keywordstyle=\color{magenta},
    numberstyle=\tiny\color{codegray},
    stringstyle=\color{codepurple},
    basicstyle=\ttfamily\footnotesize,
    breakatwhitespace=false,
    breaklines=true,
    captionpos=b,
    keepspaces=true,
    numbers=left,
    numbersep=5pt,
    showspaces=false,
    showstringspaces=false,
    showtabs=false,
    tabsize=2
}
\lstset{style=mystyle}

% Header/Footer
\pagestyle{fancy}
\fancyhf{}
\rhead{Amogh Agrawal (24b1092)}
\lhead{Tokens-to-Thought}
\rfoot{Page \thepage}

\title{
    \vspace{-1cm}
    \textbf{Tokens-to-Thought: A Contextual Transformer} \\
    \large A Four-Week Journey from NumPy to Transformers
}
\author{
    Amogh Agrawal \\
    Roll No. 24b1092
}
\date{February 2026}

\begin{document}

\maketitle

\begin{abstract}
This report documents my progression through a four-week deep learning course, starting from fundamental Python scientific computing and culminating in the implementation of a character-level Transformer model for Shakespearean text generation. Each week built upon the previous, creating a coherent path from array manipulations to attention mechanisms. The hands-on approach of implementing concepts from scratch before using frameworks proved invaluable for developing intuition about how neural networks learn.
\end{abstract}

\tableofcontents
\newpage

%==============================================================================
\section{Introduction}
%==============================================================================

The goal of this course was ambitious: understand deep learning deeply enough to implement a Transformer from scratch. Rather than jumping straight to PyTorch tutorials, we started with the building blocks---NumPy arrays, gradient descent, and single neurons---before gradually adding complexity.

This bottom-up approach meant that by the time I reached the Transformer implementation in Week 4, concepts like backpropagation and vectorized operations felt natural rather than magical. The journey can be summarized as:

\begin{center}
\begin{tabular}{cl}
\toprule
\textbf{Week} & \textbf{Focus} \\
\midrule
1 & Scientific Python: NumPy, Matplotlib, Gradient Descent \\
2 & Neural Networks from Scratch (NumPy only) \\
3 & TensorFlow: Built-in and Custom Layers \\
4 & Transformer Architecture for Text Generation \\
\bottomrule
\end{tabular}
\end{center}

%==============================================================================
\section{Week 1: Python Scientific Computing}
%==============================================================================

\subsection{NumPy Fundamentals}

The first week established fluency with NumPy, Python's cornerstone library for numerical computation. Rather than simply calling functions, I focused on understanding \textit{why} certain patterns work.

\subsubsection{Array Initialization and Reshaping}

NumPy's power lies in its ability to treat arrays as mathematical objects. I practiced multiple initialization patterns:

\begin{lstlisting}[language=Python]
# Creating a 2x3 matrix via reshape
arr = np.array([1, 2, 4, 7, 13, 21]).reshape(2, 3)

# Using modern RNG for reproducibility
rng = np.random.default_rng(seed=42)
x = rng.random((n_rows, n_columns))

# Broadcasting to create constant arrays
zeros = np.broadcast_to(np.array(0), (4, 5, 2)).copy()
ones = np.full((4, 5, 2), 1)
\end{lstlisting}

The key insight was that \texttt{reshape} doesn't copy data---it creates a new view of the same memory, which is both efficient and sometimes surprising.

\subsubsection{Vectorization}

The most important lesson from Week 1 was the dramatic performance difference between loops and vectorized operations:

\begin{center}
\begin{tabular}{lc}
\toprule
\textbf{Approach} & \textbf{Time (1000×1000 array)} \\
\midrule
Nested Python loops & $\sim$95 ms \\
NumPy vectorized & $\sim$0.8 ms \\
\bottomrule
\end{tabular}
\end{center}

This 100× speedup becomes critical when training neural networks on millions of samples.

\subsection{Data Visualization}

Using company sales data, I completed three visualization exercises that emphasized Matplotlib's object-oriented API over the stateful pyplot interface:

\begin{enumerate}[noitemsep]
    \item \textbf{Line plot}: Monthly profit trends with styled markers and legends
    \item \textbf{Multi-line plot}: Product comparison using colormaps
    \item \textbf{Pie chart}: Annual sales distribution with percentage labels
\end{enumerate}

\subsection{Multivariate Gradient Descent}

The week concluded with implementing gradient descent to minimize:
\[
f(x, y) = x^4 + x^2y^2 - y^2 + y^4 + 6
\]

The analytical gradient is:
\[
\nabla f = \begin{pmatrix} 4x^3 + 2xy^2 \\ 4y^3 - 2y + 2x^2y \end{pmatrix}
\]

My implementation included backtracking line search---if a step increased the objective, the learning rate was halved until improvement occurred. This prevented divergence from aggressive step sizes.

%==============================================================================
\section{Week 2: Neural Networks from Scratch}
%==============================================================================

Week 2 was the conceptual heart of the course: implementing feedforward networks using only NumPy. No TensorFlow, no sklearn---just arrays and calculus.

\subsection{The Perceptron}

A perceptron computes:
\[
\hat{y} = \sigma\left(\sum_{i=1}^{n} w_i x_i + b\right)
\]

where $\sigma$ is a step function for classification. I implemented this with the perceptron learning rule: update weights only on misclassification.

\subsubsection{AND Gate: Success}

The AND gate is linearly separable, so a single perceptron learns it perfectly:

\begin{center}
\begin{tabular}{cc|c}
\toprule
$x_1$ & $x_2$ & AND \\
\midrule
0 & 0 & 0 \\
0 & 1 & 0 \\
1 & 0 & 0 \\
1 & 1 & 1 \\
\bottomrule
\end{tabular}
\end{center}

\subsubsection{XOR Gate: Failure}

XOR cannot be represented by a single linear boundary:

\begin{center}
\begin{tabular}{cc|c}
\toprule
$x_1$ & $x_2$ & XOR \\
\midrule
0 & 0 & 0 \\
0 & 1 & 1 \\
1 & 0 & 1 \\
1 & 1 & 0 \\
\bottomrule
\end{tabular}
\end{center}

No matter how long training runs, the perceptron cannot solve XOR. This motivated the need for hidden layers.

\subsection{Two-Layer Network}

To overcome the linear separability limitation, I implemented a network with one hidden layer:

\begin{align*}
\mathbf{z}^{[1]} &= \mathbf{W}^{[1]}\mathbf{x} + \mathbf{b}^{[1]} \\
\mathbf{a}^{[1]} &= \tanh(\mathbf{z}^{[1]}) \\
\mathbf{z}^{[2]} &= \mathbf{W}^{[2]}\mathbf{a}^{[1]} + \mathbf{b}^{[2]} \\
\hat{y} &= \sigma(\mathbf{z}^{[2]})
\end{align*}

\subsubsection{Backpropagation}

The gradients flow backward through the network via the chain rule:
\begin{align*}
\frac{\partial \mathcal{L}}{\partial \mathbf{W}^{[2]}} &= \frac{1}{m}(\mathbf{a}^{[1]})^T \cdot (\hat{y} - y) \\
\frac{\partial \mathcal{L}}{\partial \mathbf{W}^{[1]}} &= \frac{1}{m}\mathbf{x}^T \cdot \left[(\hat{y} - y) \cdot (\mathbf{W}^{[2]})^T \odot (1 - (\mathbf{a}^{[1]})^2)\right]
\end{align*}

The term $(1 - \mathbf{a}^2)$ is the derivative of tanh, and $\odot$ denotes element-wise multiplication.

\subsection{Logic Circuits}

With the two-layer network, I successfully implemented:

\begin{enumerate}[noitemsep]
    \item \textbf{XOR gate}: 4 hidden units, perfect accuracy after 5000 epochs
    \item \textbf{Full adder}: 6 hidden units, learns sum and carry outputs
    \item \textbf{Ripple-carry adder}: Composes full adders for multi-bit addition
\end{enumerate}

The ripple-carry adder was particularly satisfying---watching a neural network correctly compute $13 + 11 = 24$ by propagating carries through learned full adders.

%==============================================================================
\section{Week 3: TensorFlow Fundamentals}
%==============================================================================

Having built networks from scratch, Week 3 introduced TensorFlow as a practical framework. The goal was to understand what happens inside \texttt{Dense} and \texttt{Flatten} layers.

\subsection{MNIST Classification}

The MNIST dataset consists of 28×28 grayscale images of handwritten digits. A simple feedforward network achieves impressive accuracy:

\begin{lstlisting}[language=Python]
model = Sequential([
    Flatten(input_shape=(28, 28)),
    Dense(128, activation='relu'),
    Dense(10, activation='softmax')
])
\end{lstlisting}

\textbf{Result}: 97.2\% test accuracy after 5 epochs.

\subsection{Custom Layer Implementation}

To demystify TensorFlow's layers, I implemented custom versions:

\begin{lstlisting}[language=Python]
class CustomDenseReluLayer(tf.keras.layers.Layer):
    def __init__(self, units):
        super().__init__()
        self.units = units

    def build(self, input_shape):
        self.w = self.add_weight(
            shape=(input_shape[-1], self.units),
            initializer='glorot_uniform',
            trainable=True
        )
        self.b = self.add_weight(
            shape=(self.units,),
            initializer='zeros',
            trainable=True
        )

    def call(self, inputs):
        return tf.nn.relu(tf.matmul(inputs, self.w) + self.b)
\end{lstlisting}

The custom model achieved identical accuracy to the built-in version, confirming my implementation was correct.

\subsection{Housing Price Regression}

For the regression assignment, I compared linear and non-linear models on the California Housing dataset:

\begin{center}
\begin{tabular}{lcc}
\toprule
\textbf{Model} & \textbf{Test MSE} & \textbf{Test MAE} \\
\midrule
Linear Regression (1 neuron) & 0.52 & 0.53 \\
Feedforward NN (64→32→16→1) & 0.26 & 0.35 \\
\bottomrule
\end{tabular}
\end{center}

The neural network's ability to learn non-linear relationships cut the MSE roughly in half.

%==============================================================================
\section{Week 4: Transformer Architecture}
%==============================================================================

The final week was the culmination of everything: implementing a Transformer model to generate Shakespearean text. This required understanding attention mechanisms, positional encodings, and autoregressive generation.

\subsection{Problem Setup}

Given a corpus of Shakespeare's works ($\sim$40,000 lines), train a character-level language model that predicts the next character given a context window. The model learns to generate text that mimics Shakespeare's style.

\subsection{Architecture Overview}

The model consists of:
\begin{enumerate}[noitemsep]
    \item Token + Position Embeddings
    \item 4 Transformer Blocks
    \item Linear projection to vocabulary logits
\end{enumerate}

\subsubsection{Positional Encoding}

Unlike RNNs, Transformers have no inherent notion of sequence order. Positional embeddings inject this information:
\[
\mathbf{e}_{\text{input}} = \mathbf{E}_{\text{token}}[x] + \mathbf{E}_{\text{pos}}[\text{position}]
\]

Both embeddings are learned during training.

\subsubsection{Causal Self-Attention}

The key innovation of Transformers is self-attention, which allows each position to attend to all others. For autoregressive generation, we apply a causal mask ensuring position $i$ only attends to positions $\leq i$:

\[
\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}} + M\right)V
\]

where $M$ is a mask with $-\infty$ for future positions.

\begin{lstlisting}[language=Python]
def causal_attention_mask(batch_size, n_dest, n_src):
    mask = tf.linalg.band_part(tf.ones((n_dest, n_src)), -1, 0)
    return tf.cast(mask, dtype=tf.bool)
\end{lstlisting}

\subsubsection{Transformer Block}

Each block contains:
\begin{enumerate}[noitemsep]
    \item Multi-head self-attention (8 heads)
    \item Residual connection + Layer normalization
    \item Feed-forward network (GELU activation)
    \item Residual connection + Layer normalization
\end{enumerate}

\subsection{Training}

\begin{center}
\begin{tabular}{ll}
\toprule
\textbf{Hyperparameter} & \textbf{Value} \\
\midrule
Context length (block size) & 128 \\
Embedding dimension & 256 \\
Attention heads & 8 \\
Transformer blocks & 4 \\
Dropout & 0.1 \\
Batch size & 32 \\
Epochs & 15 \\
\bottomrule
\end{tabular}
\end{center}

\subsection{Text Generation}

Generation proceeds autoregressively:
\begin{enumerate}[noitemsep]
    \item Start with a seed sequence of length \texttt{block\_size}
    \item Run forward pass to get next-token probabilities
    \item Sample from the distribution (temperature-controlled)
    \item Append sampled token, shift window, repeat
\end{enumerate}

Temperature ($\tau$) controls randomness:
\[
p_i = \frac{\exp(z_i / \tau)}{\sum_j \exp(z_j / \tau)}
\]

\begin{itemize}[noitemsep]
    \item $\tau = 0.5$: Conservative, more repetitive
    \item $\tau = 0.8$: Balanced creativity
    \item $\tau = 1.0$: More varied output
\end{itemize}

\subsection{Sample Output}

\textbf{Prompt}: ``To be or not to be''

\textbf{Generated} ($\tau = 0.7$):
\begin{quote}
\textit{To be or not to be the cause of all my heart, And therefore I will not be so much as I am, For I have seen the time that I have been a man of such a nature that I have no more to say...}
\end{quote}

While not perfect, the model captures Shakespearean cadence and vocabulary.

%==============================================================================
\section{Reflections and Key Takeaways}
%==============================================================================

\subsection{What Worked Well}

\begin{enumerate}
    \item \textbf{Bottom-up learning}: Implementing backpropagation from scratch made TensorFlow's abstractions meaningful rather than magical.
    
    \item \textbf{Incremental complexity}: Each week built directly on the previous, creating a coherent narrative.
    
    \item \textbf{Concrete exercises}: Logic gates and adders provided immediate feedback on whether implementations were correct.
\end{enumerate}

\subsection{Challenges Faced}

\begin{enumerate}
    \item \textbf{Numerical instability}: Early gradient descent implementations diverged until I added gradient clipping and careful initialization.
    
    \item \textbf{Debugging attention}: The causal mask was tricky to get right---off-by-one errors led to information leakage.
    
    \item \textbf{Training time}: The Transformer required GPU acceleration; CPU training was prohibitively slow.
\end{enumerate}

\subsection{Future Directions}

This course provided a foundation for several next steps:
\begin{itemize}[noitemsep]
    \item Implementing word-level rather than character-level models
    \item Exploring different positional encoding schemes (sinusoidal, RoPE)
    \item Training larger models with more data
    \item Fine-tuning pre-trained models for specific tasks
\end{itemize}

%==============================================================================
\section{Conclusion}
%==============================================================================

Over four weeks, I progressed from basic array operations to implementing a working Transformer. The journey reinforced that deep learning, despite its complexity, rests on surprisingly simple foundations: matrix multiplication, differentiation, and iterative optimization.

The most valuable insight was understanding \textit{why} things work, not just \textit{how} to call APIs. When the Transformer finally generated coherent Shakespearean prose, it felt less like magic and more like the natural consequence of the principles learned in Weeks 1 through 3.

\vspace{1cm}
\hrule
\vspace{0.5cm}
\textit{Repository}: \url{https://github.com/amoghagrawal/Tokens-to-Thought-A-Contextual-Transformer}

\end{document}
